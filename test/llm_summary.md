# Keywords
['Transformer', 'self-attention', 'machine translation', 'encoder-decoder architecture', 'BLEU score', 'parallelization', 'positional encoding', 'multi-head attention', 'training efficiency', 'language modeling', 'neural networks', 'convolutional neural networks', 'training data', 'English constituency parsing', 'regularization techniques']

# Summary
## 概要
本論文では、従来のシーケンストランスダクションモデルが使用していた複雑なリカレントまたは畳込神経ネットワークに代わって、シンプルなネットワークアーキテクチャ「Transformer」を提案します。このアーキテクチャは、再帰や畳み込みを完全に排除し、注目機構のみに基づいています。2つの機械翻訳タスクにおける実験では、提案モデルが質が優れていることが示され、多くの並列化ができ、トレーニングにかかる時間が大幅に短縮されることも確認されました。特に、WMT 2014の英独翻訳タスクでは、28.4のBLEUスコアを達成し、従来の最良モデルを超える結果を示しました。また、英仏翻訳タスクでも新たな単一モデルのSTATE-of-the-ARTなBLEUスコア41.8を達成しました。さらに、Transformerは他のタスクにもよく一般化することができ、特に英語の構文解析のタスクに対しても成功裏に適用されました。

## 新規性・差分
本論文では、主に従来のシーケンス処理モデルが複雑な再帰型ニューラルネットワーク（RNN）や畳み込みニューラルネットワーク（CNN）に基づいているのに対し、新たに提案されたTransformerモデルが全ての処理を注意機構（Attention Mechanism）のみに依存している点が新規性として挙げられます。これにより、モデルは再帰的な構造を排除しつつ、並列処理を大幅に向上させることが可能になりました。実験結果から、Transformerは中間表現の品質を向上させるだけでなく、短時間での学習が可能であり、特に長いシーケンスに対して高い精度を実現しました。

### 従来手法との違い
従来のモデルは、入力と出力のシーケンスを処理する際に、時間的な計算順序に従って符号位置で計算を行うため、並列化が困難でした。これに対して、Transformerは自己注意機構を使用することで、全シーケンスの位置を一度に処理し、並列化を実現しています。これにより、計算速度が大幅に向上し、従来のRNNやCNNに比べて効率的なトレーニングを可能にしています。

### 提案手法のメリット
Transformerの最大のメリットは、その高い並列化能力です。これによって、大規模なデータセットに対しても迅速にトレーニングでき、多様なタスクに適用可能です。特に、機械翻訳の結果は、従来の最良モデルを超える精度を示し、学習にかかるコストも大幅に削減されることが確認されました。また、自己注意機構により、長距離依存関係の学習が容易になり、構文的な解析や要約作成といった異なるタスクでも優れた性能を発揮します。

### 提案手法のデメリット
一方で、Transformerにはいくつかのデメリットも存在します。特に、モデルのサイズが大きくなるにつれて、計算コストが増加しやすい点が挙げられます。また、注意機構が動作するためには、計算量がシーケンスの長さに対して二次的に増加するため、特に長いシーケンスを扱う際にはメモリ効率の観点から課題が残ります。さらなる研究が必要ですが、これらの問題を解決するためには、制限付きの自己注意や、並列化に関する新しいアプローチが求められます。

## 手法
本論文では、Transformerという新しいネットワークアーキテクチャを提案しています。これは、既存のシーケンス変換モデルが依存している複雑な再帰型ニューラルネットワーク（RNN）や畳み込みニューラルネットワーク（CNN）ではなく、注意機構のみに基づいています。この手法によって、再帰性や畳み込みを排除し、より並列化しやすくなるとともに、訓練に要する時間も大幅に削減されます。実験の結果、翻訳タスクにおいて従来の最先端モデルを上回る性能を達成しています。
このセクションでは、Transformerの基本的なアーキテクチャ、自己注意機構、エンコーダー・デコーダーのスタック、注意機構の詳細、ポジショナルエンコーディング、フィードフォワードネットワークについて詳述します。

### モデルアーキテクチャ
Transformerは、エンコーダーとデコーダーの構造を持ち、各エンコーダーおよびデコーダーにはN=6の同一層がスタックされています。各層は、マルチヘッド自己注意メカニズムと完全結合のフィードフォワードネットワークから構成されています。これにより、全体的な処理速度が向上し、トレーニングをより迅速に行えるようになっています。

### 自己注意機構
自己注意機構は、同一のシーケンス内の異なる位置を関連付けて、シーケンスの表現を計算するメカニズムです。これにより、モデルは入力シーケンス内の長距離依存性を効率よく学習できます。自己注意は、全位置間の依存関係を明示的にモデル化するため、情報の流れを改善し、訓練を迅速に行うことが可能になります。

### エンコーダー・デコーダースタック
エンコーダーは、入力シーケンスを連続表現にマッピングします。デコーダーはその連続表現を使って出力シーケンスを生成します。各デコーダー層は、エンコーダーからの出力に基づく注意のサブ層を持ち、これにより全ての入力位置にアクセスすることができ、自律的に出力を生成します。

### ポジショナルエンコーディング
Transformerは再帰や畳み込みを使用していないため、シーケンス内のトークンの相対的位置を示す情報を注入する必要があります。このために、入力埋め込みの位置に基づく情報を追加しています。具体的には、異なる周波数の正弦波と余弦波を使用してポジショナルエンコーディングを設計し、この値をエンコーディングに組み合わせて位置情報を提供しています。

### フィードフォワードネットワーク
各層にフィードフォワードネットワークが含まれており、これは各位置に対して個別かつ同一に適用されます。これは2つの線形変換からなり、その合間にReLU活性化を挿入します。これにより、入出力次元が固定され、各層で異なるパラメータを使用することが可能となります。

## 結果
本研究ではTransformerモデルがWMT 2014の英語-ドイツ語および英語-フランス語翻訳タスクにおいて、従来の最先端モデルを大幅に上回る性能を示したことを報告している。具体的には、"Transformer (big)"モデルは英語-ドイツ語翻訳タスクで新たなBLEUスコア28.4を達成し、これまでの最高スコアを2.0以上改善した。また、英語-フランス語翻訳タスクにおいてもBLEUスコア41.8を達成し、競合モデルの四分の一以下のトレーニングコストであった。このように、Transformerは従来の手法と比較してトレーニング時間を短縮し、高い翻訳精度を実現している。

加えて、Transformerは他のタスクにおいても優れた一般化能力を示した。特に、英語の構文解析タスクにおいても良好な結果を出しており、モデルの反復計算から外れたアプローチの有効性を裏付けている。

### 機械翻訳の実験結果
TransformerモデルはWMT 2014の英語-ドイツ語翻訳タスクにおいて、最終的なBLEUスコア28.4を達成し、従来の最先端モデルを上回った。特に、トレーニングには8台のNVIDIA P100 GPUを使用し、3.5日で完了。このモデルは、トレーニングコストを考慮に入れると、非常に効率的であった。また、英語-フランス語翻訳タスクでもBLEUスコア41.8を達成し、これは従来の単一モデルとしての最も優れた結果であった。さらに、トレーニング時にはドロップアウト率0.1を適用し、過剰適合を防ぐための正則化手法が有効であったことが示された。

### モデルの変種と性能比較
著者は、Transformerの異なる構成要素がモデルの性能に与える影響を評価するため、いくつかのモデル変種を検討した。その結果、Attentionヘッドの数を減少させることはモデルの品質を低下させることが分かった。また、アテンションのキーサイズを減少させると、モデルのパフォーマンスが悪化することも示唆された。さらに、大きいモデルほど性能が向上する傾向が確認でき、ドロップアウトが過剰適合を防ぐ上で効果的であることが再確認された。

### 英語の構文解析
英語の構文解析タスクにおいても、Transformerは優れた性能を示し、特に「Wall Street Journal」データセットを使用した際、91.3のF1スコアを達成。半教師あり設定でもモデルの性能が向上し、92.7のスコアを記録したことから、Transformerの一般化能力が証明された。このことで研究者達は、言語の文法構造を捉える能力に関しても、Transformerモデルの優位性を示すことができた。
